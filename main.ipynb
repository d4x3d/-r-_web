{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Nigerian Accented English ASR - Main Notebook\n",
                "\n",
                "This notebook handles:\n",
                "1. Setup and Authentication\n",
                "2. Downloading YouTube videos as audio\n",
                "3. Loading the NCAIR1/NigerianAccentedEnglish model\n",
                "4. Testing the model\n",
                "5. Quantization and ONNX Conversion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q yt-dlp torch torchaudio transformers librosa optimum onnx onnxruntime accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "# Login to Hugging Face (required for gated model)\n",
                "login(new_session=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import librosa\n",
                "import yt_dlp\n",
                "import os\n",
                "\n",
                "# Check device\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use a pipeline as a high-level helper\n",
                "from transformers import pipeline\n",
                "\n",
                "# Initialize pipeline (optional usage)\n",
                "# pipe = pipeline(\"automatic-speech-recognition\", model=\"NCAIR1/NigerianAccentedEnglish\", device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model directly\n",
                "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
                "\n",
                "print(\"Loading model...\")\n",
                "processor = AutoProcessor.from_pretrained(\"NCAIR1/NigerianAccentedEnglish\")\n",
                "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"NCAIR1/NigerianAccentedEnglish\")\n",
                "model.to(device)\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_youtube_audio(url, output_name=\"test_audio\"):\n",
                "    print(f\"Downloading audio from {url}...\")\n",
                "    ydl_opts = {\n",
                "        'format': 'bestaudio/best',\n",
                "        'postprocessors': [{\n",
                "            'key': 'FFmpegExtractAudio',\n",
                "            'preferredcodec': 'wav',\n",
                "            'preferredquality': '192',\n",
                "        }],\n",
                "        'outtmpl': output_name,\n",
                "    }\n",
                "    \n",
                "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
                "        ydl.download([url])\n",
                "    \n",
                "    return f\"{output_name}.wav\"\n",
                "\n",
                "def transcribe_audio(audio_path):\n",
                "    print(f\"Transcribing {audio_path}...\")\n",
                "    # Load audio\n",
                "    audio, sr = librosa.load(audio_path, sr=16000)\n",
                "    \n",
                "    # Process\n",
                "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    # Generate\n",
                "    with torch.no_grad():\n",
                "        generated_ids = model.generate(inputs.input_features)\n",
                "    \n",
                "    # Decode\n",
                "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "    return transcription"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example usage:\n",
                "# url = \"YOUR_YOUTUBE_URL_HERE\"\n",
                "# audio_file = download_youtube_audio(url)\n",
                "# text = transcribe_audio(audio_file)\n",
                "# print(\"Transcription:\", text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quantization and ONNX Conversion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dynamic Quantization (PyTorch)\n",
                "print(\"Quantizing model (PyTorch)...\")\n",
                "model_cpu = model.cpu()\n",
                "quantized_model = torch.quantization.quantize_dynamic(\n",
                "    model_cpu,\n",
                "    {torch.nn.Linear},\n",
                "    dtype=torch.qint8\n",
                ")\n",
                "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
                "print(\"Saved quantized_model.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to ONNX\n",
                "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
                "\n",
                "print(\"Converting to ONNX...\")\n",
                "ort_model = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
                "    \"NCAIR1/NigerianAccentedEnglish\",\n",
                "    export=True,\n",
                "    provider=\"CPUExecutionProvider\"\n",
                ")\n",
                "\n",
                "ort_model.save_pretrained(\"onnx_models\")\n",
                "processor.save_pretrained(\"onnx_models\")\n",
                "print(\"Saved ONNX models to onnx_models/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantize ONNX for Mobile\n",
                "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"Quantizing ONNX models for mobile...\")\n",
                "encoder_path = Path(\"onnx_models/encoder_model.onnx\")\n",
                "decoder_path = Path(\"onnx_models/decoder_model.onnx\")\n",
                "\n",
                "if encoder_path.exists():\n",
                "    quantize_dynamic(\n",
                "        str(encoder_path),\n",
                "        \"onnx_models/encoder_model_quantized.onnx\",\n",
                "        weight_type=QuantType.QUInt8\n",
                "    )\n",
                "    print(\"Quantized encoder.\")\n",
                "\n",
                "if decoder_path.exists():\n",
                "    quantize_dynamic(\n",
                "        str(decoder_path),\n",
                "        \"onnx_models/decoder_model_quantized.onnx\",\n",
                "        weight_type=QuantType.QUInt8\n",
                "    )\n",
                "    print(\"Quantized decoder.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}